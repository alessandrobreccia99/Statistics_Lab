---
title: "Assign4"
output: html_document
date: "2023-04-29"
---

Ex 1

```{r}
conf <- function(x,delta,c){
  p <- 0
  j <- 0
  low <- 0
  high <- 0
  Z = sum(x*delta)
  for (i in x){
    p <- p+i*delta/Z
    if (p>=(1-c)/2){
      low <- j
      break
    }
    j <- j+1
  }
  p <- 0
  j <- 0
  for (i in x){
    p <- p+i*delta/Z
    if (p>=c+(1-c)/2){
      high <- j
      break
    }
    j <- j+1
  }
  return(c(low*delta,high*delta))
}

med <- function(x,delta){
  p <- 0
  j <- 0
  m <- 0
  Z = sum(x*delta)
  for (i in x){
    p <- p+i*delta/Z
    if (p>=0.5){
      m <- j
      break
    }
    j <- j+1
  }
  return(m*delta)
}
PartFunc <- function(x,delta){
  return(sum(x*1/delta))
}
```

```{r}
library(bayestestR)

grid <- 0.001
counts <- c(5, 8, 4, 6, 11, 6, 6, 5, 6, 4)
x <- seq(from = 0.001, to = 20, by = grid)
Upost <- dgamma(x,shape = sum(counts)+1, scale = 1/length(counts))
Jpost <- dgamma(x,shape = sum(counts)+1/2, scale = 1/length(counts))

cat("The confidence interval for the posterior of the Unif prior is ",conf(Upost,grid, 0.95),"\nThe mean is",sum(x*Upost*grid),"the median is",med(Upost,grid),"the variance is",sum(x*x*Upost*grid)-sum(x*Upost*grid)*sum(x*Upost*grid),"\n")

cat("The confidence interval for the posterior of the Jeffrey prior is ",conf(Jpost,grid, 0.95),"\nThe mean is",sum(x*Jpost*grid),"the median is",med(Jpost,grid),"the variance is",sum(x*x*Jpost*grid)-sum(x*Jpost*grid)*sum(x*Jpost*grid),"\n")

Umax_ind <- which.max(Upost)
Jmax_ind <- which.max(Jpost)
Usig <- 1/sqrt(-1*(log(Upost[Umax_ind-1])-2*log(Upost[Umax_ind])+log(Upost[Umax_ind+1]))/(grid**2))
Jsig <- 1/sqrt(-1*(log(Jpost[Jmax_ind-1])-2*log(Jpost[Jmax_ind])+log(Jpost[Jmax_ind+1]))/(grid**2))

UGauss <- dnorm(x,mean= Umax_ind*grid ,sd= Usig)
JGauss <- dnorm(x,mean= Jmax_ind*grid ,sd= Jsig)

xx <- seq(0,20,1)
lik <- dpois(x = xx, lambda = mean(counts))
po <- lik/sum(lik)
plot(xx,po)
```

```{r}
plot(x,UGauss, col="red",type="l",lwd=2 ,lty=2, xlim= c(3,10))
abline(v=conf(UGauss,grid, 0.95), col=c("red", "red"), lty=c(2,2), lwd=c(2, 2))
lines(x,Jpost, col="blue",lty=1,lwd=2)
abline(v=conf(Jpost,grid, 0.95), col=c("blue", "blue"), lty=c(3,3), lwd=c(2, 2))
lines(x,JGauss, col="darkgreen",lty=3,lwd=2)
abline(v=conf(JGauss,grid, 0.95), col=c("darkgreen", "darkgreen"), lty=c(2,2), lwd=c(2, 2))
lines(x,Upost, col="black",lty=1,lwd=2)
abline(v=conf(Upost,grid, 0.95), col=c("black", "black"), lty=c(2,2), lwd=c(2, 2))

legend(x="topright",legend=c("Uniform-Posterior","Jeffrey-Posterior","Uniform-Gauss-approx","Jeffrey-Gauss-approx"), col=c("black","blue","red","darkgreen"),lty=c(1,1,2,2), cex=0.8)
```

Ex 2

```{r}
Betapar <- function(mean, var) {
  shape1 <- ((1 - mean) / var - 1 / mean) * mean ^ 2
  shape2 <- shape1 * (1 / mean - 1)
  return(c(shape1,shape2))
}
PartFunc <- function(x,delta){
  return(sum(x*delta))
}
conf_tails <- function(x,delta,c,tail) {
  if(tail=="lower"){
    p <- 0
    j <- 0
    val <- 0
    val <- 0
    Z = sum(x*delta)
      for (i in x){
        p <- p+i*delta/Z
        if (p>=(1-c)){
          val <- j
          break
        }
        j <- j+1
      }
  }
  else if(tail=="higher"){
  p <- 0
  j <- 0
  val <- 0
  val <- 0
  Z = sum(x*delta)
      for (i in x){
        p <- p+i*delta/Z
        if (p>=c){
          val <- j
          break
        }
        j <- j+1
      }
  }
  return(val*delta)
}


N <- 10000
prob <- seq(from =0.001, to =N, by=1)/N
n <- 75
r <- 6
fe <- r/n
beta_mean <- 0.15
beta_std <- 0.14

Bprior <- dbeta(prob, shape1 = Betapar(beta_mean,beta_std*beta_std)[1], shape2 = Betapar(beta_mean,beta_std*beta_std)[2])
#Bprior <- dbeta(prob, 1, 1)
like <- dbinom(x=r, size=n, prob)

Zb <- PartFunc(like*Bprior, 1/N)
Bpost <- like*Bprior/Zb
Bfirst <- sum(prob*Bpost*1/N)
Bsecond <- sum(prob*prob*Bpost*1/N)

```

```{r}

plot(prob,Bpost,col="black",type="l",lwd=2 ,lty=1, xlim=c(0,0.3),xlab = "Probability",ylab = "Probability Density")
lines(prob,Bprior,col="orange",type="l",lwd=1 ,lty=1)
abline(v=c(Bfirst,Bfirst-sqrt(Bsecond-Bfirst*Bfirst),Bfirst+sqrt(Bsecond-Bfirst*Bfirst),conf_tails(Bpost,1/N,0.95,"higher"),0.15), col=c("blue","red", "red","darkgreen","black"), lty=c(1,2,2,2,2), lwd=c(2, 1, 1,1,1))
legend(x="topright",legend=c("Posterior","Prior","Mean","std","95% U-Tail Confidence","Old method mean"), col=c("black","orange","blue","red","darkgreen","black"),lty=c(1,1,1,2,2,2), cex=0.8)
cat("The value representing the 95% upper tail limit value is:",conf_tails(Bpost,1/N,0.95,"higher"), "so we reject the null hypothesis of p compatible with 0.15 and we accept the hypothesis 1, so the new method is better than the old one")

```

```{r}

real_freq <- rbinom(n=1000000, size=75, 0.15)
h <- hist(real_freq, breaks = max(real_freq) ,plot=FALSE)
h$counts <- cumsum(h$counts)
h$counts <- h$counts/h$counts[length(h$counts)]

plot(h, col = "lightblue", xaxt="n" ,xlim=c(-1,max(real_freq)), main = "Cumulative Function of Binomial (size=75,p=0.15)")

axis(side=1,at=h$mids,labels=seq(min(real_freq),max(real_freq)-1))
axis(side=2,at=seq(-1,1,0.1),labels=seq(-1,1,0.1))
abline(h=0.05, col="black", lty=2, lwd= 2)

cat("The null hypothsis of p= 0.15 is accepted, because we can see that 6 extractions over 75 trial is inside the 95% confidence level, because it is above the vertical line drawn at 0.05, so the new method is not better than the old one")
```

Ex 3

```{r}

p.log.like <- function(a,b,data){
  LogL <- 0.0
  for (x in data){
    LogL <- LogL + log( b/(b^2 + (x-a)^2))
  }
  return(LogL)
}
evol <- function(alpha,beta,data){
      for (i in 1:length(beta)){
        if (i==1){
          loglike <- p.log.like(alpha,beta[i],data)
        }
        else{
          loglike <- rbind(loglike,p.log.like(alpha,beta[i],data))
        }
      }
      post <- exp(loglike)/(sum(0.2*0.2*exp(loglike)))
      alpha.max.index <- which(post == max(post), arr.ind = TRUE)[2]
      beta.max.index <- which(post == max(post), arr.ind = TRUE)[1]
      
      contour(beta, alpha, post, nlevels = 5,lwd = 1, xlab="beta", ylab="alpha", main = paste("Data size:",length(data) ) )
      abline(v=beta_real,h=alpha_real,col="grey")
}

```

```{r}

alpha_real <- 1
beta_real <- 2
data <- c()
for (i in 1:200){
  y <- runif(1,-3.1415/2,3.1415/2)
  data <- append(data, beta_real*tan(y)+alpha_real )
}
plot(data,seq(1,length(data),1), xlab = "x", ylab = "trials")

alpha <- seq(from =-5, to= 5, by=0.2)
beta <- seq(from =0.1, to=8.1 , by=0.2)
ndata <- c(1,2,3,10,50,100,150,200)

par(mfrow=c(2,4)) 
for (i in ndata){
  evol(alpha,beta,data[1:i])
}

par(mfrow=c(1,1)) 
evol(alpha,beta,data[1:length(data)])
```

Ex 4

```{r}
signal <- function(x, a, b, x0, w, t){ 
  t * (a*exp(-(x-x0)^2/(2*w^2)) + b)
}
log.post <- function(d, x, a, b, x0, w, t) {
        if(a<0 || b <0) {return(-Inf)} # the effect of the prior 
        sum(dpois(d, lambda=signal(x, a, b, x0, w, t), log=TRUE))
}

# Define model parameters
x0 <- 0
w_list <- c(0.1,0.25,1,2,3)
A.true <- 2 
B.true <- 1 
Delta.t <- 5
set.seed(205)

for(w in w_list){ 
      xdat <- seq(from=-7*w, to=7*w, by=0.5*w)
      s.true <- signal(xdat, A.true, B.true, x0, w, Delta.t) 
      ddat <- rpois(length(s.true), s.true)
      xplot <- seq(from=min(xdat), to=max(xdat), by=0.05*w) 
      splot <- signal(xplot, A.true, B.true, x0, w, Delta.t) 
      xdat.off <- xdat-0.25*w
      plot(xdat.off, ddat, type="s",col="firebrick3",lwd=2, main=paste("Signal width W:",w))
      par(new=TRUE)
      lines(xplot, splot, xlab="x", ylab="Signal+Background counts") 
      
      
      # - Sampling grid for computing posterior
      alim <- c(0.0, 4.0)
      blim <- c(0.5, 3)
      Nsamp <- 500
      uniGrid <- seq(from=1/(2*Nsamp), to=1-1/(2*Nsamp), by=1/Nsamp) 
      delta_a <- diff(alim)/Nsamp 
      delta_b <- diff(blim)/Nsamp
      a <- alim[1] + diff(alim)*uniGrid 
      b <- blim[1] + diff(blim)*uniGrid
      
      # Compute log unnormalized posterior, z = ln Pˆ*(a,b|D), on a regular grid
      z <- matrix(data=NA, nrow=length(a), ncol=length(b))
      
      for(j in 1:length(a)) {
       for(k in 1:length(b)) {
      z[j,k] <- log.post(ddat, xdat, a[j], b[k], x0, w, Delta.t)
      } }
      z <- z - max(z) # set maximum to zero
      # Plot unnormalized 2D posterior as contours. 
      contour(a, b, exp(z), nlevels = 5, labcex = 0.5, lwd = 2, xlab="amplitude A", ylab="background B",main=paste("Signal width W:",w))
      abline(v=A.true,h=B.true,col="grey")
}
print("We can say that the best results are obtained with a signal width around the unit (i.e w=0.25/1), even if with other values it has been obtained good results, which are compatible with the true values")
```

```{r}

ratio <- c(0.2,0.5,1,2,3,4)
x0 <- 0
w <- 1 
A.true <- 1 
Delta.t <- 10

for(r in ratio){ 
      B.true <- A.true*r
      xdat <- seq(from=-7*w, to=7*w, by=0.5*w)
      s.true <- signal(xdat, A.true, B.true, x0, w, Delta.t) 
      ddat <- rpois(length(s.true), s.true)
      xplot <- seq(from=min(xdat), to=max(xdat), by=0.05*w) 
      splot <- signal(xplot, A.true, B.true, x0, w, Delta.t) 
      xdat.off <- xdat-0.25*w
      plot(xdat.off, ddat, type="s",col="firebrick3",lwd=2, main=paste("A and B:",paste(A.true,",",B.true)))
      par(new=TRUE)
      lines(xplot, splot, xlab="x", ylab="Signal+Background counts") 
      
      
      # - Sampling grid for computing posterior
      alim <- c(0.0, 6.0)
      blim <- c(0, 5)
      Nsamp <- 400
      uniGrid <- seq(from=1/(2*Nsamp), to=1-1/(2*Nsamp), by=1/Nsamp) 
      delta_a <- diff(alim)/Nsamp 
      delta_b <- diff(blim)/Nsamp
      a <- alim[1] + diff(alim)*uniGrid 
      b <- blim[1] + diff(blim)*uniGrid
      
      # Compute log unnormalized posterior, z = ln Pˆ*(a,b|D), on a regular grid
      z <- matrix(data=NA, nrow=length(a), ncol=length(b))
      
      for(j in 1:length(a)) {
         for(k in 1:length(b)) {
             z[j,k] <- log.post(ddat, xdat, a[j], b[k], x0, w, Delta.t)
      } }
      z <- z - max(z) # set maximum to zero
      # Plot unnormalized 2D posterior as contours. 
      contour(a, b, exp(z), nlevels = 5, labcex = 0.5, lwd = 2, xlab="amplitude A", ylab="background B",main=paste("A and B:",paste(A.true,",",B.true)))
        abline(v=A.true,h=B.true,col="grey")
}

print("As long as the ration A/B is smaller than 1, the result are very accurate and precise, while, as B increases, the pdf over A and B becomes wider with a most probable value of the parameters (A,B) less compatible with the real values")
```

```{r}

t <- seq(0,20,0.01)

e <- dexp(x = t, rate = 1/3)
barplot(e, names=t)

print(sum(e*0.01))

cu <- cumsum(e*0.01)/cumsum(e*0.01)[length(e)]

print(cu[which(t==7)]-cu[which(t==3)])

cat(pexp(q = 7, rate = 1/3)-pexp(q = 3, rate =1/3))

```
